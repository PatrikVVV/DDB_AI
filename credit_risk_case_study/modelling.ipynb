{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('ddb_ml_analysis': conda)",
   "metadata": {
    "interpreter": {
     "hash": "22886844d03d05437dbb143181697e7f50898b0638617c0ae6b2977daba4ec36"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Predicting credit default with SVM\n",
    "\n",
    "In this notebook we show how one can predict whether a customer will default on the repayment of their credit card loans using support vector machines. We will use the prepared dataset from the EDA notebook for this."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from plotnine import *\n",
    "import os\n",
    "import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" # to make jupyter print all outputs, not just the last one\n",
    "from IPython.core.display import HTML # to pretty print pandas df and be able to copy them over (e.g. to ppt slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we read in the feather file with the input features\n",
    "credit_card_df = pd.read_feather(os.path.join('..', 'dataset', 'modelling_dataset.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  id  limit_bal sex education marriage  age  pay_1  pay_2  pay_3  pay_4  ...  \\\n",
       "0  1      20000   2         2        1   24      2      2     -1     -1  ...   \n",
       "1  2     120000   2         2        2   26     -1      2      0      0  ...   \n",
       "2  3      90000   2         2        2   34      0      0      0      0  ...   \n",
       "3  4      50000   2         2        1   37      0      0      0      0  ...   \n",
       "4  5      50000   1         2        1   57     -1      0     -1      0  ...   \n",
       "\n",
       "   bill_amt6  pay_amt1  pay_amt2  pay_amt3  pay_amt4  pay_amt5  pay_amt6  \\\n",
       "0          0         0       689         0         0         0         0   \n",
       "1       3261         0      1000      1000      1000         0      2000   \n",
       "2      15549      1518      1500      1000      1000      1000      5000   \n",
       "3      29547      2000      2019      1200      1100      1069      1000   \n",
       "4      19131      2000     36681     10000      9000       689       679   \n",
       "\n",
       "   default  max_delay  freq_delay  \n",
       "0        1          2           2  \n",
       "1        1          2           5  \n",
       "2        0          0           6  \n",
       "3        0          0           6  \n",
       "4        0          0           4  \n",
       "\n",
       "[5 rows x 27 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>limit_bal</th>\n      <th>sex</th>\n      <th>education</th>\n      <th>marriage</th>\n      <th>age</th>\n      <th>pay_1</th>\n      <th>pay_2</th>\n      <th>pay_3</th>\n      <th>pay_4</th>\n      <th>...</th>\n      <th>bill_amt6</th>\n      <th>pay_amt1</th>\n      <th>pay_amt2</th>\n      <th>pay_amt3</th>\n      <th>pay_amt4</th>\n      <th>pay_amt5</th>\n      <th>pay_amt6</th>\n      <th>default</th>\n      <th>max_delay</th>\n      <th>freq_delay</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>20000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>24</td>\n      <td>2</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>689</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>120000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>26</td>\n      <td>-1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3261</td>\n      <td>0</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>90000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>34</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>15549</td>\n      <td>1518</td>\n      <td>1500</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>5000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>50000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>37</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>29547</td>\n      <td>2000</td>\n      <td>2019</td>\n      <td>1200</td>\n      <td>1100</td>\n      <td>1069</td>\n      <td>1000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>50000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>57</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>19131</td>\n      <td>2000</td>\n      <td>36681</td>\n      <td>10000</td>\n      <td>9000</td>\n      <td>689</td>\n      <td>679</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 27 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "credit_card_df.head()"
   ]
  },
  {
   "source": [
    "# Creating the train, test, and validation splits\n",
    "\n",
    "- **We split the dataset into train, validation, and test**\n",
    "- We will set test aside, which will be used to evaluate the final performance of the model. Then we will use train and validation to fit the actual model.\n",
    "- Note that since our dataset is imbalanced, we must make sure that the distribution of the target across train, validation, and test is balanced, otherwise we run the risk of e.g. not have sufficient default cases in the test set, for instance.\n",
    "- The reason why we split the data into train, validation, and test is that we want to train the model and then evaluate the model accuracy on data **that the model has never seen**. This is the only way to accurately evaluate the predictive performance of a model.\n",
    "- There are no hard-and-fast rules on how you have to split the data into train, validation, and test, only rules of thumb. The general idea is that you should have enough training examples to minimize the variance (uncertainty) on the model parameters, and enough test examples to minimize the variance (uncertainty) on the model performance metrics\n",
    "- If there is enough data, the following algorithm to split into train/validation/test is a good one:\n",
    "    - split the dataset into fitting data (80%) and test set (20%)\n",
    "    - split the fitting data further into train set (80% of the fitting data) and validation set (20% of the fitting data)\n",
    "- If there is not enough data, then crossvalidation is the only option in order to find the best model parameters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTestSplitter(object):\n",
    "    '''Class to perform the split of the data into train, test, and validation.\n",
    "    '''\n",
    "    def __init__(self, train_frac=0.8, validation_frac=0.2, seed=1234):\n",
    "        self.train_frac = train_frac\n",
    "        self.validation_frac = validation_frac\n",
    "        self.seed = seed\n",
    "    \n",
    "    def calculate_statistics(self):\n",
    "        statistics = {}\n",
    "        for i in ['train_set', 'test_set', 'validation_set']:\n",
    "            split_stats = {}\n",
    "            default_count = (getattr(self, i).groupby('default').size().reset_index())\n",
    "            split_stats['N_defaults'] = (default_count.loc[lambda x: x.default ==1, 0].iloc[0])\n",
    "            split_stats['percentage_total_defaults'] = split_stats['N_defaults']/self.total_n_defaults * 100\n",
    "            split_stats['N_not_defaults'] = default_count.loc[lambda x: x.default == 0, 0].iloc[0]\n",
    "            split_stats['percentage_total_not_defaults'] = split_stats['N_not_defaults']/self.total_n_not_defaults * 100\n",
    "            statistics[i] = split_stats\n",
    "        self.split_statistics = statistics\n",
    "\n",
    "    def split_train_test(self, df):\n",
    "        print(\"Generating the train/validation/test splits...\")\n",
    "        self.total_n_defaults = df.loc[lambda x: x.default == 1].shape[0]\n",
    "        self.total_n_not_defaults = df.loc[lambda x: x.default == 0].shape[0]\n",
    "        self.train_set = df.sample(frac=self.train_frac, random_state=self.seed)\n",
    "        self.test_set = df.loc[lambda x: ~x.id.isin(self.train_set.id)].reset_index(drop=True)\n",
    "        self.validation_set = self.train_set.sample(frac=self.validation_frac).reset_index(drop=True)\n",
    "        self.train_set = self.train_set.loc[lambda x: ~x.id.isin(self.validation_set.id)].reset_index(drop=True)\n",
    "        print(\"calculating the statistics...\")\n",
    "        self.calculate_statistics()\n",
    "        print(\"split completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generating the train/validation/test splits...\ncalculating the statistics...\nsplit completed\n"
     ]
    }
   ],
   "source": [
    "# create a fitting_splits object that will hold the train, validation, and test data\n",
    "fitting_splits = TrainTestSplitter()\n",
    "fitting_splits.split_train_test(credit_card_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6000, 27)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "fitting_splits.test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'train_set': {'N_defaults': 4281,\n",
       "  'percentage_total_defaults': 64.5117540687161,\n",
       "  'N_not_defaults': 14919,\n",
       "  'percentage_total_not_defaults': 63.85464817668207},\n",
       " 'test_set': {'N_defaults': 1307,\n",
       "  'percentage_total_defaults': 19.695599758890896,\n",
       "  'N_not_defaults': 4693,\n",
       "  'percentage_total_not_defaults': 20.086457798322204},\n",
       " 'validation_set': {'N_defaults': 1048,\n",
       "  'percentage_total_defaults': 15.792646172393008,\n",
       "  'N_not_defaults': 3752,\n",
       "  'percentage_total_not_defaults': 16.05889402499572}}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "fitting_splits.split_statistics"
   ]
  },
  {
   "source": [
    "# Dummification and scaling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Before fitting a model to our data, we need to deal with two data transformations that are **essential** to be able to obtain a performant model. These are **dummification** or **one-hot-encoding** and **scaling**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Dummification\n",
    "- **Dummification** is used to transform categorical variables into numerical variables, since most machine learning models can only deal with numerical variables. You probably already encountered this in your statistics course since linear regression requires dummification.\n",
    "- In dummification, we transform a categorical variable with N values into C_0, ..., C_N columns that take only 0 or 1 values. A row then has exactly one of these columns with 1, and the others with 0, namely, the column corresponding to the original value of the categorical variable for that row.\n",
    "- Note that there are alternative ways to deal with categorical variables. One is to use ordinal encoding, which simply encodes the categorical variable as integers. In the above dataset, the categorical variables of education, gender, marriage, etc. are ordinally encoded\n",
    "- Ordinal encoding, however, introduces an order between the categories which doesn't necessarily make sense and can cause problems for ML algorithms. For instance, if we have two categories \"male\" and \"female\", and we encode them as 0 and 1, then we are also encoding that \"male\" < \"female\" since 0 < 1. Which does not make sense and can lead to issues with the models, for instance predictions in-between classes\n",
    "- In our dataset, one might argue that while there is a natural order for education, no such order exists for sex and marriage. Therefore, we are going to one-hot encode the sex and marriage variables\n",
    "\n",
    "We will use the sklearn one hot encoder to encode the sex and marriage variables. The education variable we will leave as an ordinal (integer) variable. See\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html for documentation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Scaling\n",
    "- **feature scaling** refers to the application of a transformation to the numerical variables in the dataset which normalizes the range of the numerical variables, ensuring that these ranges are the same\n",
    "- this is performed because often numerical variables can have wildly different ranges; for instance, the \"age\" variable in the above dataset has a very different range from the \"limit_bal\" variable.\n",
    "- this can cause problems to machine learning models, in particular those, like SVM, that rely on measuring distances in e.g. Euclidean spaces. Without scaling, features whose range is in the high numers (e.g. limit_bal) will come to dominate the distance measures used in the algorithms, and features with ranges in the low numbers (e.g. age) will have little to no impact on the model\n",
    "- there are many ways to standardize variables (https://en.wikipedia.org/wiki/Feature_scaling). The most common are:\n",
    "    - min/max scaling\n",
    "    - mean normalization\n",
    "    - standardization (z-score calculation)\n",
    "\n",
    "sklearn provides various classes to perform feature scaling. We are going to use the StandardScaler, which performs z-score standardization. See https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html for documentation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "metadata": {},
     "execution_count": 361
    }
   ],
   "source": [
    "# 1. fit a one hot encoder transformer to the TRAIN SET. The object we obtain will be reused later, to transform our data\n",
    "# note: it is important that these transformers are fitted only on the train data.\n",
    "one_hot_encoder = OneHotEncoder() # one hot encoder is a class instance\n",
    "one_hot_encoder.fit(fitting_splits.train_set[['sex', 'marriage']]) # we can fit the encoder instance on the columns that need to be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[array(['1', '2'], dtype=object), array(['0', '1', '2', '3'], dtype=object)]"
      ]
     },
     "metadata": {},
     "execution_count": 362
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['sex_1', 'sex_2', 'marriage_0', 'marriage_1', 'marriage_2',\n",
       "       'marriage_3'], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 362
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       sex_1  sex_2  marriage_0  marriage_1  marriage_2  marriage_3\n",
       "0        1.0    0.0         0.0         1.0         0.0         0.0\n",
       "1        1.0    0.0         0.0         0.0         1.0         0.0\n",
       "2        0.0    1.0         0.0         1.0         0.0         0.0\n",
       "3        0.0    1.0         0.0         1.0         0.0         0.0\n",
       "4        0.0    1.0         0.0         1.0         0.0         0.0\n",
       "...      ...    ...         ...         ...         ...         ...\n",
       "19195    0.0    1.0         0.0         0.0         1.0         0.0\n",
       "19196    0.0    1.0         0.0         0.0         1.0         0.0\n",
       "19197    1.0    0.0         0.0         0.0         1.0         0.0\n",
       "19198    0.0    1.0         0.0         0.0         1.0         0.0\n",
       "19199    0.0    1.0         0.0         0.0         1.0         0.0\n",
       "\n",
       "[19200 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sex_1</th>\n      <th>sex_2</th>\n      <th>marriage_0</th>\n      <th>marriage_1</th>\n      <th>marriage_2</th>\n      <th>marriage_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19195</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19196</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19197</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19198</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19199</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>19200 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 362
    }
   ],
   "source": [
    "# let's check what the encoded did\n",
    "one_hot_encoder.categories_ # after we fit the encoder, the instance learns which values are present in each data column.\n",
    "encoded_names = one_hot_encoder.get_feature_names_out() # with this method we can retrieve the names of the new dummy columns that have been computed\n",
    "encoded_names\n",
    "encoded_categories = one_hot_encoder.transform(fitting_splits.train_set[['sex', 'marriage']]).toarray() # at this point, we can use the fitted encoder to transform any array with a sex and marriage column; not just the training set, but also the validation or test set. The encoder will take that array of shape (M,2) and transform it to an array of shape (M, N), where N is the total number of distinct possible values for the encoded features. The values of the new array will be either 0 or 1, encoding whether that value of the feature applies to the row.\n",
    "df_encoded = pd.DataFrame(encoded_categories)\n",
    "df_encoded.columns = encoded_names\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          id  limit_bal education  age  pay_1  pay_2  pay_3  pay_4  pay_5  \\\n",
       "0      13126     400000         1   34     -2     -2     -2     -2     -2   \n",
       "1      14636      80000         2   34      0      0      0      0      0   \n",
       "2      19430     200000         3   49      1     -2     -1     -1     -1   \n",
       "3       4382      20000         2   41     -1     -1     -1     -1     -1   \n",
       "4       7660      70000         1   36      2      0      0      0      0   \n",
       "...      ...        ...       ...  ...    ...    ...    ...    ...    ...   \n",
       "19195  18960     300000         2   30     -1     -1     -1     -1     -1   \n",
       "19196    254     160000         1   28      0      0      0      0     -1   \n",
       "19197  11838      30000         2   29      6      5      4      3      2   \n",
       "19198   6120      50000         2   24      2      0      0      2      2   \n",
       "19199   4692      80000         1   25     -1     -1     -2     -2     -2   \n",
       "\n",
       "       pay_6  ...  pay_amt6  default  max_delay  freq_delay  sex_1  sex_2  \\\n",
       "0         -2  ...         0        0         -2           0    1.0    0.0   \n",
       "1          0  ...      2000        0          0           6    1.0    0.0   \n",
       "2         -1  ...         0        0          1           1    0.0    1.0   \n",
       "3         -1  ...         0        0         -1           0    0.0    1.0   \n",
       "4          0  ...         0        0          2           6    0.0    1.0   \n",
       "...      ...  ...       ...      ...        ...         ...    ...    ...   \n",
       "19195      2  ...      7705        0          2           1    0.0    1.0   \n",
       "19196      0  ...     10000        0          0           5    0.0    1.0   \n",
       "19197      0  ...         0        1          6           6    1.0    0.0   \n",
       "19198      2  ...      2200        1          2           6    0.0    1.0   \n",
       "19199     -2  ...         0        0         -1           0    0.0    1.0   \n",
       "\n",
       "       marriage_0  marriage_1  marriage_2  marriage_3  \n",
       "0             0.0         1.0         0.0         0.0  \n",
       "1             0.0         0.0         1.0         0.0  \n",
       "2             0.0         1.0         0.0         0.0  \n",
       "3             0.0         1.0         0.0         0.0  \n",
       "4             0.0         1.0         0.0         0.0  \n",
       "...           ...         ...         ...         ...  \n",
       "19195         0.0         0.0         1.0         0.0  \n",
       "19196         0.0         0.0         1.0         0.0  \n",
       "19197         0.0         0.0         1.0         0.0  \n",
       "19198         0.0         0.0         1.0         0.0  \n",
       "19199         0.0         0.0         1.0         0.0  \n",
       "\n",
       "[19200 rows x 31 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>limit_bal</th>\n      <th>education</th>\n      <th>age</th>\n      <th>pay_1</th>\n      <th>pay_2</th>\n      <th>pay_3</th>\n      <th>pay_4</th>\n      <th>pay_5</th>\n      <th>pay_6</th>\n      <th>...</th>\n      <th>pay_amt6</th>\n      <th>default</th>\n      <th>max_delay</th>\n      <th>freq_delay</th>\n      <th>sex_1</th>\n      <th>sex_2</th>\n      <th>marriage_0</th>\n      <th>marriage_1</th>\n      <th>marriage_2</th>\n      <th>marriage_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>13126</td>\n      <td>400000</td>\n      <td>1</td>\n      <td>34</td>\n      <td>-2</td>\n      <td>-2</td>\n      <td>-2</td>\n      <td>-2</td>\n      <td>-2</td>\n      <td>-2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-2</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>14636</td>\n      <td>80000</td>\n      <td>2</td>\n      <td>34</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>2000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>6</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>19430</td>\n      <td>200000</td>\n      <td>3</td>\n      <td>49</td>\n      <td>1</td>\n      <td>-2</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4382</td>\n      <td>20000</td>\n      <td>2</td>\n      <td>41</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7660</td>\n      <td>70000</td>\n      <td>1</td>\n      <td>36</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19195</th>\n      <td>18960</td>\n      <td>300000</td>\n      <td>2</td>\n      <td>30</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2</td>\n      <td>...</td>\n      <td>7705</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19196</th>\n      <td>254</td>\n      <td>160000</td>\n      <td>1</td>\n      <td>28</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>10000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19197</th>\n      <td>11838</td>\n      <td>30000</td>\n      <td>2</td>\n      <td>29</td>\n      <td>6</td>\n      <td>5</td>\n      <td>4</td>\n      <td>3</td>\n      <td>2</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>6</td>\n      <td>6</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19198</th>\n      <td>6120</td>\n      <td>50000</td>\n      <td>2</td>\n      <td>24</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>...</td>\n      <td>2200</td>\n      <td>1</td>\n      <td>2</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>19199</th>\n      <td>4692</td>\n      <td>80000</td>\n      <td>1</td>\n      <td>25</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-2</td>\n      <td>-2</td>\n      <td>-2</td>\n      <td>-2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>19200 rows Ã— 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 363
    }
   ],
   "source": [
    "# after we have generated the dummy columns, we want to drop the original columns from the training data, and replace them with the dummy columns\n",
    "train_set = fitting_splits.train_set.drop(['sex', 'marriage'], axis=1)\n",
    "train_set = pd.concat([train_set, df_encoded], axis=1)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "metadata": {},
     "execution_count": 364
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.21449296,  1.77779765, -1.07899959, ...,  1.09375214,\n",
       "        -1.06692209, -0.10439709],\n",
       "       [-0.04047304, -0.67618793,  0.18622529, ..., -0.91428392,\n",
       "         0.93727556, -0.10439709],\n",
       "       [ 0.51201136,  0.24405666,  1.45145016, ...,  1.09375214,\n",
       "        -1.06692209, -0.10439709],\n",
       "       ...,\n",
       "       [-0.36292848, -1.05962317,  0.18622529, ..., -0.91428392,\n",
       "         0.93727556, -0.10439709],\n",
       "       [-1.02189925, -0.90624908,  0.18622529, ..., -0.91428392,\n",
       "         0.93727556, -0.10439709],\n",
       "       [-1.18646907, -0.67618793, -1.07899959, ..., -0.91428392,\n",
       "         0.93727556, -0.10439709]])"
      ]
     },
     "metadata": {},
     "execution_count": 364
    }
   ],
   "source": [
    "# now we fit the StandardScaler, in order to standardize all the features\n",
    "standard_scaler = StandardScaler()\n",
    "standard_scaler.fit(train_set)\n",
    "# the above standard scaler, when apply to a dataset with the same columns as the train set, will scale all the numerical features and return a numpy array\n",
    "standard_scaler.transform(train_set)"
   ]
  },
  {
   "source": [
    "Once we have a one hot encoder and a standard scaler fitted on the training data, it is handy to combine all the transformations needed to prepare a dataset with the same schema as the training dataset into a class. This is so that we can apply it to training/validation/test, and any new dataset with the same columns that we might get in the future."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparer(object):\n",
    "\n",
    "    def __init__(self, one_hot_encoder, standard_scaler):\n",
    "        self.one_hot_encoder = one_hot_encoder\n",
    "        self.standard_scaler = standard_scaler\n",
    "\n",
    "    def dummify(self, df):\n",
    "        vars_to_encode = ['sex', 'marriage']\n",
    "        df_to_encode = df[vars_to_encode]\n",
    "        df_encoded = self.one_hot_encoder.transform(df_to_encode).toarray()\n",
    "        df_encoded = pd.DataFrame(df_encoded)\n",
    "        df_encoded.columns = self.one_hot_encoder.get_feature_names_out()\n",
    "        # add the encoded columns and drop the original columns\n",
    "        df = df.drop(vars_to_encode,axis=1)\n",
    "        df = pd.concat([df, df_encoded], axis=1)\n",
    "        return df\n",
    "\n",
    "    def scale(self, df):\n",
    "        cols = df.columns\n",
    "        df = self.standard_scaler.transform(df)\n",
    "        df = pd.DataFrame(df)\n",
    "        df.columns = cols\n",
    "        return df\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        df = df.reset_index(drop=True)\n",
    "        # first dummify the data\n",
    "        df = self.dummify(df)\n",
    "        # then scale it\n",
    "        df = self.scale(df)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         id  limit_bal  education       age    pay_1     pay_2     pay_3  \\\n",
       "0 -0.214493   1.777798  -1.079000 -0.163603 -1.76117 -1.551696 -1.534925   \n",
       "1 -0.040473  -0.676188   0.186225 -0.163603  0.01807  0.113670  0.142254   \n",
       "2  0.512011   0.244057   1.451450  1.464624  0.90769 -1.551696 -0.696335   \n",
       "3 -1.222195  -1.136310   0.186225  0.596236 -0.87155 -0.719013 -0.696335   \n",
       "4 -0.844422  -0.752875  -1.079000  0.053494  1.79731  0.113670  0.142254   \n",
       "\n",
       "      pay_4     pay_5     pay_6  ...  pay_amt6   default  max_delay  \\\n",
       "0 -1.526049 -1.532662 -1.486580  ... -0.296744 -0.535355  -1.814149   \n",
       "1  0.192989  0.242087  0.259930  ... -0.181969 -0.535355  -0.324056   \n",
       "2 -0.666530 -0.645288 -0.613325  ... -0.296744 -0.535355   0.420990   \n",
       "3 -0.666530 -0.645288 -0.613325  ... -0.296744 -0.535355  -1.069102   \n",
       "4  0.192989  0.242087  0.259930  ... -0.296744 -0.535355   1.166036   \n",
       "\n",
       "   freq_delay     sex_1     sex_2  marriage_0  marriage_1  marriage_2  \\\n",
       "0   -1.634404  1.239081 -1.239081   -0.039559    1.093752   -1.066922   \n",
       "1    0.807789  1.239081 -1.239081   -0.039559   -0.914284    0.937276   \n",
       "2   -1.227371 -0.807049  0.807049   -0.039559    1.093752   -1.066922   \n",
       "3   -1.634404 -0.807049  0.807049   -0.039559    1.093752   -1.066922   \n",
       "4    0.807789 -0.807049  0.807049   -0.039559    1.093752   -1.066922   \n",
       "\n",
       "   marriage_3  \n",
       "0   -0.104397  \n",
       "1   -0.104397  \n",
       "2   -0.104397  \n",
       "3   -0.104397  \n",
       "4   -0.104397  \n",
       "\n",
       "[5 rows x 31 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>limit_bal</th>\n      <th>education</th>\n      <th>age</th>\n      <th>pay_1</th>\n      <th>pay_2</th>\n      <th>pay_3</th>\n      <th>pay_4</th>\n      <th>pay_5</th>\n      <th>pay_6</th>\n      <th>...</th>\n      <th>pay_amt6</th>\n      <th>default</th>\n      <th>max_delay</th>\n      <th>freq_delay</th>\n      <th>sex_1</th>\n      <th>sex_2</th>\n      <th>marriage_0</th>\n      <th>marriage_1</th>\n      <th>marriage_2</th>\n      <th>marriage_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.214493</td>\n      <td>1.777798</td>\n      <td>-1.079000</td>\n      <td>-0.163603</td>\n      <td>-1.76117</td>\n      <td>-1.551696</td>\n      <td>-1.534925</td>\n      <td>-1.526049</td>\n      <td>-1.532662</td>\n      <td>-1.486580</td>\n      <td>...</td>\n      <td>-0.296744</td>\n      <td>-0.535355</td>\n      <td>-1.814149</td>\n      <td>-1.634404</td>\n      <td>1.239081</td>\n      <td>-1.239081</td>\n      <td>-0.039559</td>\n      <td>1.093752</td>\n      <td>-1.066922</td>\n      <td>-0.104397</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.040473</td>\n      <td>-0.676188</td>\n      <td>0.186225</td>\n      <td>-0.163603</td>\n      <td>0.01807</td>\n      <td>0.113670</td>\n      <td>0.142254</td>\n      <td>0.192989</td>\n      <td>0.242087</td>\n      <td>0.259930</td>\n      <td>...</td>\n      <td>-0.181969</td>\n      <td>-0.535355</td>\n      <td>-0.324056</td>\n      <td>0.807789</td>\n      <td>1.239081</td>\n      <td>-1.239081</td>\n      <td>-0.039559</td>\n      <td>-0.914284</td>\n      <td>0.937276</td>\n      <td>-0.104397</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.512011</td>\n      <td>0.244057</td>\n      <td>1.451450</td>\n      <td>1.464624</td>\n      <td>0.90769</td>\n      <td>-1.551696</td>\n      <td>-0.696335</td>\n      <td>-0.666530</td>\n      <td>-0.645288</td>\n      <td>-0.613325</td>\n      <td>...</td>\n      <td>-0.296744</td>\n      <td>-0.535355</td>\n      <td>0.420990</td>\n      <td>-1.227371</td>\n      <td>-0.807049</td>\n      <td>0.807049</td>\n      <td>-0.039559</td>\n      <td>1.093752</td>\n      <td>-1.066922</td>\n      <td>-0.104397</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.222195</td>\n      <td>-1.136310</td>\n      <td>0.186225</td>\n      <td>0.596236</td>\n      <td>-0.87155</td>\n      <td>-0.719013</td>\n      <td>-0.696335</td>\n      <td>-0.666530</td>\n      <td>-0.645288</td>\n      <td>-0.613325</td>\n      <td>...</td>\n      <td>-0.296744</td>\n      <td>-0.535355</td>\n      <td>-1.069102</td>\n      <td>-1.634404</td>\n      <td>-0.807049</td>\n      <td>0.807049</td>\n      <td>-0.039559</td>\n      <td>1.093752</td>\n      <td>-1.066922</td>\n      <td>-0.104397</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.844422</td>\n      <td>-0.752875</td>\n      <td>-1.079000</td>\n      <td>0.053494</td>\n      <td>1.79731</td>\n      <td>0.113670</td>\n      <td>0.142254</td>\n      <td>0.192989</td>\n      <td>0.242087</td>\n      <td>0.259930</td>\n      <td>...</td>\n      <td>-0.296744</td>\n      <td>-0.535355</td>\n      <td>1.166036</td>\n      <td>0.807789</td>\n      <td>-0.807049</td>\n      <td>0.807049</td>\n      <td>-0.039559</td>\n      <td>1.093752</td>\n      <td>-1.066922</td>\n      <td>-0.104397</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 366
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         id  limit_bal  education       age    pay_1     pay_2     pay_3  \\\n",
       "0 -0.299083   0.474118   0.186225  0.162042  1.79731  1.779036  1.819434   \n",
       "1  0.531488  -0.906249  -1.079000  0.487688 -1.76117 -1.551696 -1.534925   \n",
       "2 -1.370976  -0.906249   0.186225 -0.272151  0.01807  0.113670  0.142254   \n",
       "3  0.417165  -0.906249  -1.079000 -1.249087  0.01807  0.113670  1.819434   \n",
       "4  1.458173  -0.906249   0.186225 -1.357636 -0.87155  0.113670  0.142254   \n",
       "\n",
       "      pay_4     pay_5     pay_6  ...  pay_amt6   default  max_delay  \\\n",
       "0  0.192989  0.242087  0.259930  ... -0.154308  1.867921   1.166036   \n",
       "1 -1.526049 -1.532662 -1.486580  ... -0.262599 -0.535355  -1.814149   \n",
       "2  0.192989  0.242087  0.259930  ... -0.239357 -0.535355  -0.324056   \n",
       "3  0.192989  0.242087  0.259930  ... -0.255425  1.867921   1.166036   \n",
       "4  1.912027  2.016836  2.006439  ... -0.296744 -0.535355   1.166036   \n",
       "\n",
       "   freq_delay     sex_1     sex_2  marriage_0  marriage_1  marriage_2  \\\n",
       "0    0.807789 -0.807049  0.807049   -0.039559    1.093752   -1.066922   \n",
       "1   -1.634404 -0.807049  0.807049   -0.039559    1.093752   -1.066922   \n",
       "2    0.807789  1.239081 -1.239081   -0.039559    1.093752   -1.066922   \n",
       "3    0.807789 -0.807049  0.807049   -0.039559    1.093752   -1.066922   \n",
       "4    0.400757 -0.807049  0.807049   -0.039559   -0.914284    0.937276   \n",
       "\n",
       "   marriage_3  \n",
       "0   -0.104397  \n",
       "1   -0.104397  \n",
       "2   -0.104397  \n",
       "3   -0.104397  \n",
       "4   -0.104397  \n",
       "\n",
       "[5 rows x 31 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>limit_bal</th>\n      <th>education</th>\n      <th>age</th>\n      <th>pay_1</th>\n      <th>pay_2</th>\n      <th>pay_3</th>\n      <th>pay_4</th>\n      <th>pay_5</th>\n      <th>pay_6</th>\n      <th>...</th>\n      <th>pay_amt6</th>\n      <th>default</th>\n      <th>max_delay</th>\n      <th>freq_delay</th>\n      <th>sex_1</th>\n      <th>sex_2</th>\n      <th>marriage_0</th>\n      <th>marriage_1</th>\n      <th>marriage_2</th>\n      <th>marriage_3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.299083</td>\n      <td>0.474118</td>\n      <td>0.186225</td>\n      <td>0.162042</td>\n      <td>1.79731</td>\n      <td>1.779036</td>\n      <td>1.819434</td>\n      <td>0.192989</td>\n      <td>0.242087</td>\n      <td>0.259930</td>\n      <td>...</td>\n      <td>-0.154308</td>\n      <td>1.867921</td>\n      <td>1.166036</td>\n      <td>0.807789</td>\n      <td>-0.807049</td>\n      <td>0.807049</td>\n      <td>-0.039559</td>\n      <td>1.093752</td>\n      <td>-1.066922</td>\n      <td>-0.104397</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.531488</td>\n      <td>-0.906249</td>\n      <td>-1.079000</td>\n      <td>0.487688</td>\n      <td>-1.76117</td>\n      <td>-1.551696</td>\n      <td>-1.534925</td>\n      <td>-1.526049</td>\n      <td>-1.532662</td>\n      <td>-1.486580</td>\n      <td>...</td>\n      <td>-0.262599</td>\n      <td>-0.535355</td>\n      <td>-1.814149</td>\n      <td>-1.634404</td>\n      <td>-0.807049</td>\n      <td>0.807049</td>\n      <td>-0.039559</td>\n      <td>1.093752</td>\n      <td>-1.066922</td>\n      <td>-0.104397</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.370976</td>\n      <td>-0.906249</td>\n      <td>0.186225</td>\n      <td>-0.272151</td>\n      <td>0.01807</td>\n      <td>0.113670</td>\n      <td>0.142254</td>\n      <td>0.192989</td>\n      <td>0.242087</td>\n      <td>0.259930</td>\n      <td>...</td>\n      <td>-0.239357</td>\n      <td>-0.535355</td>\n      <td>-0.324056</td>\n      <td>0.807789</td>\n      <td>1.239081</td>\n      <td>-1.239081</td>\n      <td>-0.039559</td>\n      <td>1.093752</td>\n      <td>-1.066922</td>\n      <td>-0.104397</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.417165</td>\n      <td>-0.906249</td>\n      <td>-1.079000</td>\n      <td>-1.249087</td>\n      <td>0.01807</td>\n      <td>0.113670</td>\n      <td>1.819434</td>\n      <td>0.192989</td>\n      <td>0.242087</td>\n      <td>0.259930</td>\n      <td>...</td>\n      <td>-0.255425</td>\n      <td>1.867921</td>\n      <td>1.166036</td>\n      <td>0.807789</td>\n      <td>-0.807049</td>\n      <td>0.807049</td>\n      <td>-0.039559</td>\n      <td>1.093752</td>\n      <td>-1.066922</td>\n      <td>-0.104397</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.458173</td>\n      <td>-0.906249</td>\n      <td>0.186225</td>\n      <td>-1.357636</td>\n      <td>-0.87155</td>\n      <td>0.113670</td>\n      <td>0.142254</td>\n      <td>1.912027</td>\n      <td>2.016836</td>\n      <td>2.006439</td>\n      <td>...</td>\n      <td>-0.296744</td>\n      <td>-0.535355</td>\n      <td>1.166036</td>\n      <td>0.400757</td>\n      <td>-0.807049</td>\n      <td>0.807049</td>\n      <td>-0.039559</td>\n      <td>-0.914284</td>\n      <td>0.937276</td>\n      <td>-0.104397</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 366
    }
   ],
   "source": [
    "data_preparer = DataPreparer(one_hot_encoder, standard_scaler)\n",
    "data_preparer.prepare_data(fitting_splits.train_set).head()\n",
    "data_preparer.prepare_data(fitting_splits.validation_set).head()"
   ]
  },
  {
   "source": [
    "# Modeling\n",
    "\n",
    "Now, we can move on to train a support vector classification model.\n",
    "\n",
    "1. First, we are going to train a default SVC model on the train set, and use it to predict on the test set. This is just to illustrate the code and the steps involved in fitting and predicting with a model using sklearn.\n",
    "2. Then we are going to evaluate the predictions that the above model generates on the test set. How is the model performing?\n",
    "3. Then, we are going to:\n",
    "    - perform a train-validation grid-search in the hyperparameter space of the SVC model (**hyperparameter tuning**)\n",
    "    - choose the best set of hyperparameters among those explored\n",
    "    - train a SVC model with the found hyperparameters on train + validation, and predict on test \n",
    "4. Finally, we are going to evaluate the predictions of the resulting model at point 3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Simple train/test model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVC(gamma='auto')"
      ]
     },
     "metadata": {},
     "execution_count": 367
    }
   ],
   "source": [
    "# get the transformed train set\n",
    "train_set_transformed = data_preparer.prepare_data(train_test_split.train_set)\n",
    "X_train = train_set_transformed.drop(['default', 'id'], axis=1) # need to drop the target! otherwise data leakage\n",
    "y_train = train_test_split.train_set['default'] # take it from the original untransformed dataset\n",
    "# create the model instance\n",
    "simple_SVC = SVC(gamma='auto')\n",
    "# fit the model instance on X_train\n",
    "simple_SVC.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "5995    0\n",
       "5996    1\n",
       "5997    1\n",
       "5998    0\n",
       "5999    0\n",
       "Length: 6000, dtype: int32"
      ]
     },
     "metadata": {},
     "execution_count": 368
    }
   ],
   "source": [
    "# now we can use the fitted simple_SVC model to predict on the test dataset!\n",
    "X_test = data_preparer.prepare_data(train_test_split.test_set).drop(['default', 'id'], axis=1)\n",
    "y_hat_test = simple_SVC.predict(X_test)\n",
    "# the result is a vector of predicted classes for the observations in validation, which the model was not trained on!\n",
    "pd.Series(y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    5345\n",
       "1     655\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 369
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    4693\n",
       "1    1307\n",
       "Name: default, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 369
    }
   ],
   "source": [
    "# we can already see that the model is underpredicting the minority class and overpredicting on the majority class\n",
    "pd.Series(y_hat_test).value_counts()\n",
    "train_test_split.test_set['default'].value_counts()"
   ]
  },
  {
   "source": [
    "### Evaluate the model performance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We can now evaluate the classification performance of the model on the test set.\n",
    "There are various possible metrics that can be useful to evaluate the model performance. In particular, the following can be considered:\n",
    "\n",
    "- accuracy score: the percentage of examples that was correctly classified (see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "    - pros: simple to interpret and communicate to non-technical people\n",
    "    - cons: it can be too simple and disguise bad performance as good performance, for instance in the case of imbalanced datasets\n",
    "\n",
    "- confusion matrix: create the 2 x 2 confusion matrix, and use it to evaluate the model misclassifications:\n",
    "   - pros: easy to interpret, visual, shows the difference in classification accuracy on the two classes\n",
    "   - cons: it does not provide a single accuracy metrics, which is useful to compare multiple models against each other\n",
    "\n",
    "<div>\n",
    "<img src=\"img/confusion_matrix.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "- evaluate the precision and recall of the classification separately:\n",
    "    - precision: the % of the examples classified as positives (defaults) that are actually positive: TP / TP + FP\n",
    "    - recall: the % of the actually positive examples (defaults) that was classified as positive: TP / TP + FN\n",
    "\n",
    "- F1-score: the harmonic mean of the precision and recall scores (see above).\n",
    "   - calculated as TP/(TP + 1/2(FP + FN)). Ranges between 0 and 1.\n",
    "   - the higher the score, the better the model\n",
    "   - weights false negatives and false positives equally (which might now always be what you want, as these can have different associated costs)\n",
    "    - pros: it combines precision and recall into a single score that can be used to evaluate models\n",
    "    - pros: realiable with imbalanced datasets\n",
    "    - cons: it can be hard to explain to stakeholders. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For the present exercise, we are going to use the F1-score to evaluate the classification accuracy. Sklearn has a handy function in the metrics module to calculate the F1 score, the f1_score function."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.44"
      ]
     },
     "metadata": {},
     "execution_count": 370
    }
   ],
   "source": [
    "round(f1_score(train_test_split.test_set['default'], y_hat_test), 2)"
   ]
  },
  {
   "source": [
    "We got 0.45 as an F1 score. Is this good? Is this bad? These are the wrong questions. An f1 score is a **relative** measure of accuracy, i.e., it is useful to compare different models against each other, and determine which one is the better classifier. But it's not useful by itself in isolation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For instance, consider the simplest possible classification model: the model that classifies everything as 0 (no default). The F1 score of this model then is:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "execution_count": 371
    }
   ],
   "source": [
    "f1_score(train_test_split.test_set['default'], np.zeros(train_test_split.test_set.shape[0]))"
   ]
  },
  {
   "source": [
    "According to the F1-score, this model sucks (it's the worst possible model), even though its accuracy might be quite high because of the imbalanced data. The F1 score is the correct measure here, but it's also telling us that the model above is a better model than this naive model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Since we have an imbalanced dataset, we might want to do a quick try and see if we can improve the performance by balancing it. The quickest way to do this is to downsample the negative class so as to achieve balance. Let's see what happens."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "default\n",
       "0    16817\n",
       "1     4783\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 372
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVC(gamma='auto')"
      ]
     },
     "metadata": {},
     "execution_count": 372
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.41"
      ]
     },
     "metadata": {},
     "execution_count": 372
    }
   ],
   "source": [
    "# check out the distribution of the class in the train set\n",
    "train_test_split.train_set.groupby('default').size()\n",
    "# we downsample the negative class, choosing the same number of observations as we have for the positive class\n",
    "# we then discard data and obtain a balanced dataset\n",
    "train_set_sampled = train_test_split.train_set.sample(4783, random_state=1234)\n",
    "\n",
    "# after this the code is the same as above, to fit and predict with the model\n",
    "train_set_transformed = data_preparer.prepare_data(train_set_sampled)\n",
    "X_train = train_set_transformed.drop(['default', 'id'], axis=1) # need to drop the target! otherwise data leakage\n",
    "y_train = train_set_sampled['default'] # take it from the original untransformed dataset\n",
    "# create the model instance\n",
    "simple_SVC = SVC(gamma='auto')\n",
    "# fit the model instance on X_train\n",
    "simple_SVC.fit(X=X_train, y=y_train)\n",
    "\n",
    "X_test = data_preparer.prepare_data(train_test_split.test_set).drop(['default', 'id'], axis=1)\n",
    "y_hat_test = simple_SVC.predict(X_test)\n",
    "# the result is a vector of predicted classes for the observations in validation, which the model was not trained on!\n",
    "round(f1_score(train_test_split.test_set['default'], y_hat_test), 2)"
   ]
  },
  {
   "source": [
    "The f1 score of the above model is only 0.42. In other words, we have lost performance by downsampling, compared to the first basic model. That means that the benefit of making the training set balanced is outweighed by the loss of training examples.\n",
    "Most likely, an approach like SMOTE (see the EDA notebook) would work better here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Hypertuning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In the above, we have fitted and predicted with basically the default SVC model. However, each machine learning model comes with a set of **hyperparameters** that can be tuned. Think of a model as a machine, and of hyperparameters as levers and dials of the machine, which need to be set before it can used. In our case, the hyperparameters must be set before we can train and then predict with the model.\n",
    "\n",
    "But how should we set these parameters? There is no way to set them from data, in the fitting process. Rather, we need to explore the space of possible values for these hyperparameters, and for each combination that we might want to try, fit a model and evaluate its predictive performance. The combination of hyperparameters values that returns the model with the highest performance is the optimal value of hyperparameters among the combination explored.\n",
    "\n",
    "Note that model hyperparameters are different from model parameters. The main difference is that the latter are learned during the fitting process, and estimated directly from the data in this process. The model hyperparameters cannot be estimated in this way because they define the \"shape\" or \"class\" of the model before the training has taken place. See also https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/ for an explanation of the difference.\n",
    "\n",
    "Often, a model will have many hyperparameters. Not all of them are important, or have the same impact on the performance of the model. So how do you determine which ones to tune? The only way is from experience, or by understanding the mathematics of the model and understanding which parameters matter the most, or by empirical research that has been done by other researchers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For SVMs, the most important parameters are:\n",
    "- The kernel. This is the actual function that maps the observations to the (higher-dimensional) space. In the above, we used the default radial kernel, which is also that which performs the best most of the times\n",
    "- for different kernels there might be different hyperparamters that can be tuned. These hyperparamters usually specify the shape of the transformation of the feature space. For the radial kernel, we have two important ones:\n",
    "- the gamma parameter\n",
    "- the C ('cost') parameter\n",
    "\n",
    "You can find a good description of what these parameters control here: https://towardsdatascience.com/svm-hyperparameters-explained-with-visualizations-143e48cb701b, but for now it suffices to understand that these are parameters that control the shape of the model, and their values can have an impact on the model performance."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We then want to tune the gamma and C parameter to see if we get a better SVC model. There are various approach to do this, but the simplest is: **grid search**. \n",
    "\n",
    "- This means that we will create a grid with different combinations of values of the two parameters\n",
    "- For each combination of hyperparameter values we fit a SVC model to the data with those hyperparameter values, and predict on the **validation** dataset\n",
    "- The model which returns the best f1-score will show us what the best hyperparameters in the grid are\n",
    "- We can then use these hyperparameters to train a final model on all the fitting data, and predict on the test set"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<div>\n",
    "<img src=\"img/hyperparameter_tuning.png\" width=\"800\">\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = [2 ** -5, 2 ** 15]\n",
    "gamma_range = [2 ** -15, 2 ** 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_grid = np.linspace(C_range[0], C_range[1], num=10)\n",
    "gamma_grid = np.linspace(gamma_range[0], gamma_range[1], num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([3.12500000e-02, 3.64091667e+03, 7.28180208e+03, 1.09226875e+04,\n",
       "       1.45635729e+04, 1.82044583e+04, 2.18453438e+04, 2.54862292e+04,\n",
       "       2.91271146e+04, 3.27680000e+04])"
      ]
     },
     "metadata": {},
     "execution_count": 393
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([3.05175781e-05, 8.88916016e-01, 1.77780151e+00, 2.66668701e+00,\n",
       "       3.55557251e+00, 4.44445801e+00, 5.33334351e+00, 6.22222900e+00,\n",
       "       7.11111450e+00, 8.00000000e+00])"
      ]
     },
     "metadata": {},
     "execution_count": 393
    }
   ],
   "source": [
    "C_grid\n",
    "gamma_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "begin hypertuning\n",
      "fitting model for C: 0.03125 and gamma: 3.0517578125e-05\n",
      "fitting complete.\n",
      "fitting model for C: 0.03125 and gamma: 0.888916015625\n",
      "fitting complete.\n",
      "fitting model for C: 0.03125 and gamma: 1.777801513671875\n",
      "fitting complete.\n",
      "fitting model for C: 0.03125 and gamma: 2.66668701171875\n",
      "fitting complete.\n",
      "fitting model for C: 0.03125 and gamma: 3.555572509765625\n",
      "fitting complete.\n",
      "fitting model for C: 0.03125 and gamma: 4.4444580078125\n",
      "fitting complete.\n",
      "fitting model for C: 0.03125 and gamma: 5.333343505859375\n",
      "fitting complete.\n",
      "fitting model for C: 0.03125 and gamma: 6.22222900390625\n",
      "fitting complete.\n",
      "fitting model for C: 0.03125 and gamma: 7.111114501953125\n",
      "fitting complete.\n",
      "fitting model for C: 0.03125 and gamma: 8.0\n",
      "fitting complete.\n",
      "fitting model for C: 3640.9166666666665 and gamma: 3.0517578125e-05\n",
      "fitting complete.\n",
      "fitting model for C: 3640.9166666666665 and gamma: 0.888916015625\n",
      "fitting complete.\n",
      "fitting model for C: 3640.9166666666665 and gamma: 1.777801513671875\n",
      "fitting complete.\n",
      "fitting model for C: 3640.9166666666665 and gamma: 2.66668701171875\n",
      "fitting complete.\n",
      "fitting model for C: 3640.9166666666665 and gamma: 3.555572509765625\n",
      "fitting complete.\n",
      "fitting model for C: 3640.9166666666665 and gamma: 4.4444580078125\n",
      "fitting complete.\n",
      "fitting model for C: 3640.9166666666665 and gamma: 5.333343505859375\n",
      "fitting complete.\n",
      "fitting model for C: 3640.9166666666665 and gamma: 6.22222900390625\n",
      "fitting complete.\n",
      "fitting model for C: 3640.9166666666665 and gamma: 7.111114501953125\n",
      "fitting complete.\n",
      "fitting model for C: 3640.9166666666665 and gamma: 8.0\n",
      "fitting complete.\n",
      "fitting model for C: 7281.802083333333 and gamma: 3.0517578125e-05\n",
      "fitting complete.\n",
      "fitting model for C: 7281.802083333333 and gamma: 0.888916015625\n",
      "fitting complete.\n",
      "fitting model for C: 7281.802083333333 and gamma: 1.777801513671875\n",
      "fitting complete.\n",
      "fitting model for C: 7281.802083333333 and gamma: 2.66668701171875\n",
      "fitting complete.\n",
      "fitting model for C: 7281.802083333333 and gamma: 3.555572509765625\n",
      "fitting complete.\n",
      "fitting model for C: 7281.802083333333 and gamma: 4.4444580078125\n",
      "fitting complete.\n",
      "fitting model for C: 7281.802083333333 and gamma: 5.333343505859375\n",
      "fitting complete.\n",
      "fitting model for C: 7281.802083333333 and gamma: 6.22222900390625\n",
      "fitting complete.\n",
      "fitting model for C: 7281.802083333333 and gamma: 7.111114501953125\n",
      "fitting complete.\n",
      "fitting model for C: 7281.802083333333 and gamma: 8.0\n",
      "fitting complete.\n",
      "fitting model for C: 10922.6875 and gamma: 3.0517578125e-05\n",
      "fitting complete.\n",
      "fitting model for C: 10922.6875 and gamma: 0.888916015625\n",
      "fitting complete.\n",
      "fitting model for C: 10922.6875 and gamma: 1.777801513671875\n",
      "fitting complete.\n",
      "fitting model for C: 10922.6875 and gamma: 2.66668701171875\n",
      "fitting complete.\n",
      "fitting model for C: 10922.6875 and gamma: 3.555572509765625\n",
      "fitting complete.\n",
      "fitting model for C: 10922.6875 and gamma: 4.4444580078125\n",
      "fitting complete.\n",
      "fitting model for C: 10922.6875 and gamma: 5.333343505859375\n",
      "fitting complete.\n",
      "fitting model for C: 10922.6875 and gamma: 6.22222900390625\n",
      "fitting complete.\n",
      "fitting model for C: 10922.6875 and gamma: 7.111114501953125\n",
      "fitting complete.\n",
      "fitting model for C: 10922.6875 and gamma: 8.0\n",
      "fitting complete.\n",
      "fitting model for C: 14563.572916666666 and gamma: 3.0517578125e-05\n",
      "fitting complete.\n",
      "fitting model for C: 14563.572916666666 and gamma: 0.888916015625\n",
      "fitting complete.\n",
      "fitting model for C: 14563.572916666666 and gamma: 1.777801513671875\n",
      "fitting complete.\n",
      "fitting model for C: 14563.572916666666 and gamma: 2.66668701171875\n",
      "fitting complete.\n",
      "fitting model for C: 14563.572916666666 and gamma: 3.555572509765625\n",
      "fitting complete.\n",
      "fitting model for C: 14563.572916666666 and gamma: 4.4444580078125\n",
      "fitting complete.\n",
      "fitting model for C: 14563.572916666666 and gamma: 5.333343505859375\n",
      "fitting complete.\n",
      "fitting model for C: 14563.572916666666 and gamma: 6.22222900390625\n",
      "fitting complete.\n",
      "fitting model for C: 14563.572916666666 and gamma: 7.111114501953125\n",
      "fitting complete.\n",
      "fitting model for C: 14563.572916666666 and gamma: 8.0\n",
      "fitting complete.\n",
      "fitting model for C: 18204.458333333332 and gamma: 3.0517578125e-05\n",
      "fitting complete.\n",
      "fitting model for C: 18204.458333333332 and gamma: 0.888916015625\n",
      "fitting complete.\n",
      "fitting model for C: 18204.458333333332 and gamma: 1.777801513671875\n",
      "fitting complete.\n",
      "fitting model for C: 18204.458333333332 and gamma: 2.66668701171875\n",
      "fitting complete.\n",
      "fitting model for C: 18204.458333333332 and gamma: 3.555572509765625\n",
      "fitting complete.\n",
      "fitting model for C: 18204.458333333332 and gamma: 4.4444580078125\n",
      "fitting complete.\n",
      "fitting model for C: 18204.458333333332 and gamma: 5.333343505859375\n",
      "fitting complete.\n",
      "fitting model for C: 18204.458333333332 and gamma: 6.22222900390625\n",
      "fitting complete.\n",
      "fitting model for C: 18204.458333333332 and gamma: 7.111114501953125\n",
      "fitting complete.\n",
      "fitting model for C: 18204.458333333332 and gamma: 8.0\n",
      "fitting complete.\n",
      "fitting model for C: 21845.34375 and gamma: 3.0517578125e-05\n",
      "fitting complete.\n",
      "fitting model for C: 21845.34375 and gamma: 0.888916015625\n",
      "fitting complete.\n",
      "fitting model for C: 21845.34375 and gamma: 1.777801513671875\n",
      "fitting complete.\n",
      "fitting model for C: 21845.34375 and gamma: 2.66668701171875\n",
      "fitting complete.\n",
      "fitting model for C: 21845.34375 and gamma: 3.555572509765625\n",
      "fitting complete.\n",
      "fitting model for C: 21845.34375 and gamma: 4.4444580078125\n",
      "fitting complete.\n",
      "fitting model for C: 21845.34375 and gamma: 5.333343505859375\n",
      "fitting complete.\n",
      "fitting model for C: 21845.34375 and gamma: 6.22222900390625\n",
      "fitting complete.\n",
      "fitting model for C: 21845.34375 and gamma: 7.111114501953125\n",
      "fitting complete.\n",
      "fitting model for C: 21845.34375 and gamma: 8.0\n",
      "fitting complete.\n",
      "fitting model for C: 25486.229166666664 and gamma: 3.0517578125e-05\n",
      "fitting complete.\n",
      "fitting model for C: 25486.229166666664 and gamma: 0.888916015625\n",
      "fitting complete.\n",
      "fitting model for C: 25486.229166666664 and gamma: 1.777801513671875\n",
      "fitting complete.\n",
      "fitting model for C: 25486.229166666664 and gamma: 2.66668701171875\n",
      "fitting complete.\n",
      "fitting model for C: 25486.229166666664 and gamma: 3.555572509765625\n",
      "fitting complete.\n",
      "fitting model for C: 25486.229166666664 and gamma: 4.4444580078125\n",
      "fitting complete.\n",
      "fitting model for C: 25486.229166666664 and gamma: 5.333343505859375\n",
      "fitting complete.\n",
      "fitting model for C: 25486.229166666664 and gamma: 6.22222900390625\n",
      "fitting complete.\n",
      "fitting model for C: 25486.229166666664 and gamma: 7.111114501953125\n",
      "fitting complete.\n",
      "fitting model for C: 25486.229166666664 and gamma: 8.0\n",
      "fitting complete.\n",
      "fitting model for C: 29127.114583333332 and gamma: 3.0517578125e-05\n",
      "fitting complete.\n",
      "fitting model for C: 29127.114583333332 and gamma: 0.888916015625\n",
      "fitting complete.\n",
      "fitting model for C: 29127.114583333332 and gamma: 1.777801513671875\n",
      "fitting complete.\n",
      "fitting model for C: 29127.114583333332 and gamma: 2.66668701171875\n",
      "fitting complete.\n",
      "fitting model for C: 29127.114583333332 and gamma: 3.555572509765625\n",
      "fitting complete.\n",
      "fitting model for C: 29127.114583333332 and gamma: 4.4444580078125\n",
      "fitting complete.\n",
      "fitting model for C: 29127.114583333332 and gamma: 5.333343505859375\n",
      "fitting complete.\n",
      "fitting model for C: 29127.114583333332 and gamma: 6.22222900390625\n",
      "fitting complete.\n",
      "fitting model for C: 29127.114583333332 and gamma: 7.111114501953125\n",
      "fitting complete.\n",
      "fitting model for C: 29127.114583333332 and gamma: 8.0\n",
      "fitting complete.\n",
      "fitting model for C: 32768.0 and gamma: 3.0517578125e-05\n",
      "fitting complete.\n",
      "fitting model for C: 32768.0 and gamma: 0.888916015625\n",
      "fitting complete.\n",
      "fitting model for C: 32768.0 and gamma: 1.777801513671875\n",
      "fitting complete.\n",
      "fitting model for C: 32768.0 and gamma: 2.66668701171875\n",
      "fitting complete.\n",
      "fitting model for C: 32768.0 and gamma: 3.555572509765625\n",
      "fitting complete.\n",
      "fitting model for C: 32768.0 and gamma: 4.4444580078125\n",
      "fitting complete.\n",
      "fitting model for C: 32768.0 and gamma: 5.333343505859375\n",
      "fitting complete.\n",
      "fitting model for C: 32768.0 and gamma: 6.22222900390625\n",
      "fitting complete.\n",
      "fitting model for C: 32768.0 and gamma: 7.111114501953125\n",
      "fitting complete.\n",
      "fitting model for C: 32768.0 and gamma: 8.0\n",
      "fitting complete.\n",
      "hypertuning complete in 300.65 minutes\n"
     ]
    }
   ],
   "source": [
    "# will hold the models fitted on train, for each hyperparameter combination\n",
    "fitted_models = []\n",
    "\n",
    "X_train = data_preparer.prepare_data(fitting_splits.train_set).drop(['default', 'id'], axis=1)\n",
    "y_train = fitting_splits.train_set[ 'default']\n",
    "\n",
    "def fit_svm_model(X_train, y_train, C, gamma):\n",
    "    # create the model instance with the required parameters\n",
    "    simple_SVC = SVC(gamma=gamma, C=C)\n",
    "    # fit the model instance on X_train\n",
    "    simple_SVC.fit(X=X_train, y=y_train)\n",
    "    return simple_SVC\n",
    "\n",
    "# train a model for each hyperparameter combo\n",
    "print(\"begin hypertuning\")\n",
    "start_time = datetime.datetime.now()\n",
    "for C in C_grid:\n",
    "    for gamma in gamma_grid:\n",
    "        # fit a model with the given hyperparameter values.\n",
    "        # A new model will be fitted for each loop cycle\n",
    "        print(f\"fitting model for C: {C} and gamma: {gamma}\")\n",
    "        fitted_model = fit_svm_model(X_train, y_train, C, gamma)\n",
    "        print(f\"fitting complete.\")\n",
    "        # store the fitted model in the fitted_model list\n",
    "        fitted_models.append(fitted_model)\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "print(f'hypertuning complete in {round((end_time - start_time).seconds/60, 2)} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the fitted models as a pickle so they can be analysed later\n",
    "with open('../dataset/hypertuning_models.pickle', 'wb') as handle:\n",
    "    pickle.dump(fitted_models, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "0.40864440078585457 is not in list",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\RICCAR~1.PIN\\AppData\\Local\\Temp/ipykernel_936/902593489.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# choose the model with the best F score value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mmax_f1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfitted_models\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_f1_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# see what values of gamma and C this model has\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 0.40864440078585457 is not in list"
     ]
    }
   ],
   "source": [
    "# evaluate the F1 score of each model on validation, and find the best hyperparameters\n",
    "X_validation = data_preparer.prepare_data(fitting_splits.validation_set).drop(['default', 'id'], axis=1)\n",
    "y_validation = fitting_splits.validation_set['default']\n",
    "\n",
    "def calculate_f1_score(X, y, model):\n",
    "    y_hat = model.predict(X)\n",
    "    return f1_score(y, y_hat)\n",
    "\n",
    "f_scores = [calculate_f1_score(X_validation, y_validation, m) for m in fitted_models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.37237643872714965,\n",
       " 0.2966666666666667,\n",
       " 0.24906132665832292,\n",
       " 0.2219269102990033,\n",
       " 0.19818562456385203,\n",
       " 0.18116462976276063,\n",
       " 0.1652046783625731,\n",
       " 0.1525804038893044,\n",
       " 0.13465952563121653,\n",
       " 0.1254841208365608,\n",
       " 0.38692461641094067,\n",
       " 0.2912300055157198,\n",
       " 0.2485946283572767,\n",
       " 0.2219269102990033,\n",
       " 0.19818562456385203,\n",
       " 0.18116462976276063,\n",
       " 0.1652046783625731,\n",
       " 0.1525804038893044,\n",
       " 0.13465952563121653,\n",
       " 0.1254841208365608,\n",
       " 0.39387890884896876,\n",
       " 0.2866629773104593,\n",
       " 0.24765478424015008,\n",
       " 0.2217795484727756,\n",
       " 0.19818562456385203,\n",
       " 0.18116462976276063,\n",
       " 0.1652046783625731,\n",
       " 0.1525804038893044,\n",
       " 0.13465952563121653,\n",
       " 0.1254841208365608,\n",
       " 0.3952254641909814,\n",
       " 0.2880886426592798,\n",
       " 0.24765478424015008,\n",
       " 0.2217795484727756,\n",
       " 0.19818562456385203,\n",
       " 0.18116462976276063,\n",
       " 0.1652046783625731,\n",
       " 0.1525804038893044,\n",
       " 0.13465952563121653,\n",
       " 0.1254841208365608,\n",
       " 0.4,\n",
       " 0.28982300884955753,\n",
       " 0.2485946283572767,\n",
       " 0.2217795484727756,\n",
       " 0.19818562456385203,\n",
       " 0.18116462976276063,\n",
       " 0.1652046783625731,\n",
       " 0.1525804038893044,\n",
       " 0.13465952563121653,\n",
       " 0.1254841208365608,\n",
       " 0.4034278180619644,\n",
       " 0.2910902047592695,\n",
       " 0.2485946283572767,\n",
       " 0.2217795484727756,\n",
       " 0.19818562456385203,\n",
       " 0.18116462976276063,\n",
       " 0.1652046783625731,\n",
       " 0.1525804038893044,\n",
       " 0.13465952563121653,\n",
       " 0.1254841208365608,\n",
       " 0.40762656147271537,\n",
       " 0.29235880398671094,\n",
       " 0.24875000000000003,\n",
       " 0.2217795484727756,\n",
       " 0.19818562456385203,\n",
       " 0.18116462976276063,\n",
       " 0.1652046783625731,\n",
       " 0.1525804038893044,\n",
       " 0.13465952563121653,\n",
       " 0.1254841208365608,\n",
       " 0.40682414698162733,\n",
       " 0.29235880398671094,\n",
       " 0.24765478424015008,\n",
       " 0.2217795484727756,\n",
       " 0.19818562456385203,\n",
       " 0.18116462976276063,\n",
       " 0.1652046783625731,\n",
       " 0.1525804038893044,\n",
       " 0.13465952563121653,\n",
       " 0.1254841208365608,\n",
       " 0.40864440078585457,\n",
       " 0.29125138427464003,\n",
       " 0.24780976220275344,\n",
       " 0.2217795484727756,\n",
       " 0.19818562456385203,\n",
       " 0.18116462976276063,\n",
       " 0.1652046783625731,\n",
       " 0.1525804038893044,\n",
       " 0.13465952563121653,\n",
       " 0.1254841208365608]"
      ]
     },
     "metadata": {},
     "execution_count": 399
    }
   ],
   "source": [
    "f_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the model with the best F score value\n",
    "max_f1_score = max(f_scores)\n",
    "best_model_index = f_scores.index(max_f1_score)\n",
    "best_model = fitted_models[best_model_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The best value of C found is 32768.0\nThe best value of gamma is 3.0517578125e-05\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best value of C found is {best_model.C}\")\n",
    "print(f\"The best value of gamma is {best_model.gamma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "now train a model with the best parameters on train + validation, and predict on test to get the final out-of-sample performance of the model at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SVC(C=32768.0, gamma=3.0517578125e-05)"
      ]
     },
     "metadata": {},
     "execution_count": 414
    }
   ],
   "source": [
    "selected_model = SVC(gamma = best_model.gamma, C = best_model.C)\n",
    "X_train_validation = pd.concat([X_train, X_validation])\n",
    "y_train_validation = pd.concat([fitting_splits.train_set['default'], fitting_splits.validation_set['default']])\n",
    "selected_model.fit(X_train_validation, y_train_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.40925828511309836"
      ]
     },
     "metadata": {},
     "execution_count": 420
    }
   ],
   "source": [
    "# now predict on test with the fitted model to get the final performance measure\n",
    "X_test = data_preparer.prepare_data(fitting_splits.test_set).drop(['default', 'id'],axis=1)\n",
    "y_test = fitting_splits.test_set['default']\n",
    "calculate_f1_score(X_test, y_test, selected_model)"
   ]
  },
  {
   "source": [
    "only 0.41 of F1 score. Not great, we didn't manage to beat the default parameters. Some further ideas to improve the SVM model:\n",
    "- refine the grid around the best parameters found above. Perhaps around the current best parameters found there are better options\n",
    "- move to random or Bayesian search rather than grid search\n",
    "- apply the smote technique to generate more examples of the minority class and train the model on the inflated data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}